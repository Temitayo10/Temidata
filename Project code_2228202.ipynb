{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da1e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e742a",
   "metadata": {},
   "source": [
    "# Data Precessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e725fd62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DATA PRECESSING\n",
    "ds = r'C:\\Users\\personal\\OneDrive - University of Bolton\\Desktop\\new data.csv'\n",
    "df = pd.read_csv(ds, encoding='latin-1')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c26e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing missing values\n",
    "df.isna().sum()\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b75b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicates\n",
    "from collections import Counter\n",
    "counter = Counter(df)\n",
    "has_duplicates = any(count > 1 for count in counter.values())\n",
    "print(has_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1881dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleanig\n",
    "import re\n",
    "import string\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].map(round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].map(round2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4829562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words, tokenization, pos-tagging and lemmatization \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in pos_tags:\n",
    "        pos_tag_simple = tag[0].lower() \n",
    "        pos_tag_simple = pos_tag_simple if pos_tag_simple in ['a', 'r', 'n', 'v'] else None\n",
    "        if pos_tag_simple:\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(token, pos=pos_tag_simple))\n",
    "        else:\n",
    "            lemmatized_tokens.append(token)\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "df['content'] = df['content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unwanted words\n",
    "words_to_remove = [\"thing\",\"get\",\"good\",\"could\", \"don t\", \"sainsbury s\", \"tesco\", \"s\", \"ve\", \"may\", \"m\", \"won t\" \"would\",\"week\",\"iceland\",\"open\",\n",
    "                   \"people\",\"give\",\"sainsbury\",\"sainsbury s\",\"name\",\"today\",\"shopping\",\"won t\", \"wasnt\",\"don t\",\"though\",\"especially\", \"£\",\"superfood\", \n",
    "                   \"well\", \"always\", \"waitrose\", \"definitely\", \"shop\",\"buy\",\"day\", \"take\", \"item\", \"like\",\"morrison\",\"coop\",\"co\",\"op\",\"jan\",\n",
    "                   \"dec\",\"cd\",\"asda\",\"aswell\",\"poundland\", \"wont\", \"ha\", \"wa\", \"really\",\"find\", \"say\",\"said\", \"didnt\", \"dont\", \"told\", \"went\", \n",
    "                   \"put\", \"asked\",\"one\",\"ive\",\"u\", \"tried\",\"tell\",\"come\",\"using\",\"around\",\"keep\",\"even\",\"someone\",\"seem\",\"trying\",\"still\",\"that\",\n",
    "                   \"morrison\",\"aldi\", \"tesco\",\"sainsbury\",\"lidl\",\"asda\",\"abel\",\"cole\" , \"either\",\"right\",\"arrived\",\"lady\",\"large\",\"look\",\"look\",\"looking\",\n",
    "                   \"due\",\"least\",\"nothing\",\"till\",\"every\",\"use\",\"ask\",\"bit\",\"go\",\"going\",\"working\",\"please\",\"id\",\"came\",\"gone\",\"two\",\"actually\",\"getting\",\n",
    "                   \"im\",\"end\",\"given\",\"away\",\"yet\",\"another\",\"left\",\"happened\",\"couldnt\",\"given\",\"think\",\"cant\", \"gave\",\"reply\",\"able\", \"instead\",\"etc\",\n",
    "                   \"later\",\"help\",\"man\",\"see\",\"quite\",\"go\",\"absolutely\",\"arrived\",\"done\",\"everything\",\"contact\",\"last\",\"first\",\"thought\",\"despite\",\"back\",\n",
    "                   \"sometime\",\"never\",\"ok\",\"see\",\"make\",\"almost\",\"sainsbury\",\"le\",\"let\",\"star\",\"seems\",\"got\",\"given\",\"started\",\"reason\",\"awful\",\"company\", \n",
    "                   \"need\",\"without\",\"know\",\"found\",\"Lidls\", \"sainsburys\", \"supermarket\", \"store\"]\n",
    "remove_words = lambda x: ' '.join([word for word in x.split() if word.lower() not in words_to_remove])\n",
    "df['content'] = df['content'].apply(remove_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520a6f3",
   "metadata": {},
   "source": [
    "# Word frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae692be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for all customer reviews\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "all_text = ' '.join(df['content'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='brown').generate(all_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Customer Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0301002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart for all customer reviews\n",
    "all_text = ' '.join(df['content'])\n",
    "word_list = all_text.split()\n",
    "word_freq = pd.Series(word_list).value_counts()\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(word_freq.index[:15], word_freq.values[:15])\n",
    "plt.title('Top 10 Word Frequencies Across All Customer Reviews')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc0eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar charts for top words in all selected supermarkets\n",
    "selected_supermarkets = ['Aldi', 'Lidl', 'Asda', 'Tesco', 'Morrison','Sainsbury', \"AbelnCole\"]\n",
    "def generate_bar_chart(supermarket):\n",
    "    supermarket_data = df[df['Supermarkets'] == supermarket]\n",
    "    text_data = ' '.join(supermarket_data['content'])\n",
    "    word_list = text_data.split()\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(word_freq.index[:10], word_freq.values[:10])\n",
    "    plt.title(f'Top 10 Word Frequencies for {supermarket.capitalize()}')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "for supermarket in selected_supermarkets:\n",
    "    generate_bar_chart(supermarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compound Bar charts across the supermarkets\n",
    "selected_supermarkets = ['Aldi', 'Lidl', 'Asda', 'Tesco', 'Morrison','Sainsbury']\n",
    "superstore_texts = df[df['Supermarkets'].isin(selected_supermarkets)].groupby('Supermarkets')['content'].apply(lambda x: ' '.join(x)).to_frame()\n",
    "superstore_texts['word_list'] = superstore_texts['content'].apply(lambda x: x.split())\n",
    "word_freq_dict = {}\n",
    "for superstore, word_list in superstore_texts[['word_list']].iterrows():\n",
    "    word_freq = pd.Series(word_list['word_list']).value_counts()\n",
    "    word_freq_dict[superstore] = word_freq\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for superstore, word_freq in word_freq_dict.items():\n",
    "    plt.bar(word_freq.index[:10], word_freq.values[:10], label=superstore)\n",
    "plt.title('Top 10 Word Frequencies for Each Superstore')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar chart for relevant adjectives across the supermarkets \n",
    "words_to_count = [\"quick\", \"fast\", 'quality', 'rude', 'helpful', 'friendly', \"polite\", 'cheap', \"expensive\", \"price\", \"discount\", \"voucher\"]\n",
    "selected_supermarkets = ['Aldi', 'Sainsbury', 'Morrison', 'Lidl', 'Asda', 'AbelnCole', 'Tesco']\n",
    "df['content_lower'] = df['content'].str.lower()\n",
    "def count_words(text, word):\n",
    "    return text.count(word)\n",
    "word_counts = {}\n",
    "df_selected = df[df['Supermarkets'].isin(selected_supermarkets)]\n",
    "for word in words_to_count:\n",
    "    df_selected[word] = df_selected['content_lower'].apply(count_words, word=word)\n",
    "    word_counts[word] = df_selected.groupby('Supermarkets')[word].sum()\n",
    " for each word\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, word in enumerate(words_to_count, start=1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    plt.bar(word_counts[word].index, word_counts[word].values)\n",
    "    plt.title( f'Word: {word}', fontsize=16)\n",
    "    plt.xlabel('Supermarket', fontsize=15)\n",
    "    plt.ylabel('Word Count', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    for tick in plt.gca().get_xticklabels():\n",
    "        if tick.get_text() in selected_supermarkets:\n",
    "            tick.set_fontsize(13)  \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "word_counts_df = pd.DataFrame(word_counts)\n",
    "print(word_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84476706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of predefined topics\n",
    "content = df['content']\n",
    "groups = {\n",
    "    'Delivery service': ['deliver', 'driver', 'fast', 'wait', 'time', 'collect', 'cancel'],\n",
    "    'Customer service': ['customer', 'staff', 'manager', 'easy', 'rude', 'helpful', 'friendly', 'polite'],\n",
    "    'Online experience': ['email', 'order', 'call', 'phone', 'online'],\n",
    "    'Product quality': ['food', 'product', 'organic', 'quality', 'brand', 'stock', 'rotten', 'veg', 'fruit', 'fresh', 'produce', 'vegetable'],\n",
    "    'Payment process': ['queue', 'checkout', 'self', 'receipt', 'card', 'pay', 'refund'],\n",
    "    'Price': ['price', 'offer', 'discount', 'voucher', 'money', 'value', 'cheap', 'expensive']\n",
    "    \n",
    "}\n",
    "group_frequencies = {group: sum(content.str.contains('|'.join(words), case=False)) for group, words in groups.items()}\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(group_frequencies.values(), labels=group_frequencies.keys(), autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Frequency of Pre-defined topics in Customer Review')\n",
    "plt.axis('equal')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of selected supermarkets\n",
    "import seaborn as sns\n",
    "selected_supermarkets = ['Aldi', 'Sainsbury', 'Morrison', 'Lidl', 'Asda', 'AbelnCole', 'Tesco']\n",
    "df['content_lower'] = df['content'].str.lower()\n",
    "words_to_count = ['delivery',\"quick\", \"fast\",  'order', 'quality', 'service', 'customer', 'rude', 'helpful', 'friendly', \"polite\",'price', 'cheap',\"expensive\", \"voucher\", \"discount\"]\n",
    "word_counts_df = pd.DataFrame(index=words_to_count, columns=selected_supermarkets)\n",
    "def count_words(text, word):\n",
    "    return text.count(word)\n",
    " supermarket and word\n",
    "for supermarket in selected_supermarkets:\n",
    "    df_supermarket = df[df['Supermarkets'] == supermarket]\n",
    "    for word in words_to_count:\n",
    "        count = df_supermarket['content_lower'].apply(count_words, word=word).sum()\n",
    "        word_counts_df.at[word, supermarket] = count\n",
    "word_counts_df = word_counts_df.apply(pd.to_numeric)\n",
    "correlation_matrix = word_counts_df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Word Counts for Selected Supermarkets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9787ab5",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating topics with topic modelling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000) \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['content'])\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42) \n",
    "lda.fit(tfidf_matrix)\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topics[\"Topic {}\".format(topic_idx + 1)] = ' '.join(top_words)\n",
    "    return pd.DataFrame(topics, index=['Top Words'])\n",
    "no_top_words = 10 \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "topics_df = display_topics(lda, tfidf_vectorizer.get_feature_names_out(), no_top_words)\n",
    "print(\"Topics generated by LDA:\")\n",
    "print(topics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48c07e",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e92204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df['sentiment_scores'] = df['content'].apply(lambda x: sid.polarity_scores(x))\n",
    "df['sentiment_score'] = df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "def categorize_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "df['sentiment_category'] = df['sentiment_score'].apply(categorize_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of sentiment category\n",
    "sentiment_counts = df['sentiment_category'].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['green', 'red', 'blue'])\n",
    "plt.title('Frequency of Sentiment Categories')\n",
    "plt.xlabel('Sentiment Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels if needed\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of top words based on the sentiment analysis\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(df['content'])\n",
    "words_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "words_df['sentiment_category'] = df['sentiment_category']\n",
    "words_df = words_df[words_df['sentiment_category'] != 'neutral']\n",
    "sentiment_word_counts = words_df.groupby('sentiment_category').sum()\n",
    "top_positive_words = sentiment_word_counts.loc['positive'].nlargest(15)\n",
    "top_negative_words = sentiment_word_counts.loc['negative'].nlargest(15)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Positive sentiment\n",
    "axs[0].barh(top_positive_words.index[::-1], top_positive_words.values[::-1], color='green')\n",
    "axs[0].set_title('Top 15 Common Words for Positive Sentiment')\n",
    "axs[0].set_xlabel('Frequency')\n",
    "axs[0].set_ylabel('Words')\n",
    "\n",
    "# Negative sentiment\n",
    "axs[1].barh(top_negative_words.index[::-1], top_negative_words.values[::-1], color='red')\n",
    "axs[1].set_title('Top 15 Common Words for Negative Sentiment')\n",
    "axs[1].set_xlabel('Frequency')\n",
    "axs[1].set_ylabel('Words')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57236895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of selected sets of words based on the sentiment category across the selected supermarkets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "set1 = ['queue', 'checkout', 'self', 'pay', 'price', 'discount', 'voucher', 'refund', 'money', 'value']\n",
    "set2 = ['delivery','driver', 'fast', 'quick','wait','bag', 'box']\n",
    "set3 = ['email' 'online','call', 'phone','order', 'service','staff','manager']\n",
    "set4 = ['quality', 'organic', 'product', 'food']\n",
    "selected_supermarkets =  [\"Aldi\",\"Asda\",\"Sainsbury\",\"Morrison\", \"Tesco\", \"Lidl\", \"AbelnCole\"]\n",
    "df_filtered = Morrison[Morrison['Supermarkets'].isin(selected_supermarkets)]\n",
    "def plot_bar_chart(words, title):\n",
    "    df_words_filtered = df_filtered[df_filtered['content'].str.contains('|'.join(words), case=False)]\n",
    "    vectorizer = CountVectorizer(vocabulary=words, lowercase=False)\n",
    "    X = vectorizer.fit_transform(df_words_filtered['content'])\n",
    "    words_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    words_df['sentiment_category'] = df_words_filtered['sentiment_category']\n",
    "    words_df['Supermarkets'] = df_words_filtered['Supermarkets']\n",
    "    sentiment_word_counts = words_df.groupby(['Supermarkets', 'sentiment_category']).sum()\n",
    "    for supermarket in selected_supermarkets:\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            # Positive sentiment bars\n",
    "            plt.barh(words, sentiment_word_counts.loc[(supermarket, 'positive')], color='green', label='Positive')\n",
    "\n",
    "            # Negative sentiment bars\n",
    "            plt.barh(words, -sentiment_word_counts.loc[(supermarket, 'negative')], color='darkred', label='Negative')\n",
    "\n",
    "            plt.title(f'{title} - {supermarket}')\n",
    "            plt.xlabel('Frequency')\n",
    "            plt.ylabel('Words')\n",
    "            plt.legend()\n",
    "            plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "            plt.show()\n",
    "        except KeyError:\n",
    "            print(f\"No data available for {supermarket}\")\n",
    "\n",
    "# Plot bar charts for each set of words and each supermarket\n",
    "plot_bar_chart(set1, 'Distribution of Positive and Negative Sentiment')\n",
    "plot_bar_chart(set2, 'Distribution of Positive and Negative Sentiment')\n",
    "plot_bar_chart(set3, 'Distribution of Positive and Negative Sentiment')\n",
    "plot_bar_chart(set4, 'Distribution of Positive and Negative Sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart of predefined topics based on assigned related words\n",
    "content = df['content']\n",
    "\n",
    "# Define the groups of words\n",
    "groups = {\n",
    "    'Delivery service': ['deliver', 'driver', 'fast', 'wait', 'time', 'collect', 'cancel'],\n",
    "    'Customer service': ['customer', 'staff', 'manager', 'easy', 'rude', 'helpful', 'friendly', 'polite'],\n",
    "    'Online experience': ['email', 'order', 'call', 'phone', 'online'],\n",
    "    'Product quality': ['food', 'product', 'organic', 'quality', 'brand', 'stock', 'rotten', 'veg', 'fruit', 'fresh', 'produce', 'vegetable'],\n",
    "    'Payment process': ['queue', 'checkout', 'self', 'receipt', 'card', 'pay', 'refund'],\n",
    "    'Price': ['price', 'offer', 'discount', 'voucher', 'money', 'value', 'cheap', 'expensive']\n",
    "    \n",
    "}\n",
    "\n",
    "group_frequencies = {group: sum(content.str.contains('|'.join(words), case=False)) for group, words in groups.items()}\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(group_frequencies.values(), labels=group_frequencies.keys(), autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Frequency of Pre-defined topics in Customer Review')\n",
    "plt.axis('equal')  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf103b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word cloud of sentiment category\n",
    "from wordcloud import WordCloud\n",
    "positive_text = ' '.join(df[df['sentiment_category'] == 'positive']['content'])\n",
    "neutral_text = ' '.join(df[df['sentiment_category'] == 'neutral']['content'])\n",
    "negative_text = ' '.join(df[df['sentiment_category'] == 'negative']['content'])\n",
    "\n",
    "def generate_word_cloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color ='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_word_cloud(positive_text, \"Positive Sentiment Word Cloud\")\n",
    "generate_word_cloud(neutral_text, \"Neutral Sentiment Word Cloud\")\n",
    "generate_word_cloud(negative_text, \"Negative Sentiment Word Cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf77e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_supermarkets = ['Aldi', 'Lidl', 'Asda', 'Tesco', 'Morrison','Sainsbury', \"AbelnCole\", \"Co-op\", \"Waitrose\"]\n",
    "def generate_bar_chart(supermarket):\n",
    "    supermarket_data = df[df['Supermarkets'] == supermarket]\n",
    "    text_data = ' '.join(supermarket_data['content'])\n",
    "    word_list = text_data.split()\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(word_freq.index[:10], word_freq.values[:10])\n",
    "    plt.title(f'Top 10 Word Frequencies for {supermarket.capitalize()}')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for supermarket in selected_supermarkets:\n",
    "    generate_bar_chart(supermarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_supermarkets = ['Aldi', 'Lidl', 'Asda', 'Tesco', 'Morrison', 'Sainsbury', \"AbelnCole\", \"Co-op\", \"Waitrose\"]\n",
    "def generate_word_frequency_table(supermarket):\n",
    "    supermarket_data = df[df['Supermarkets'] == supermarket]\n",
    "    text_data = ' '.join(supermarket_data['content'])\n",
    "    word_list = text_data.split()\n",
    "    word_freq = pd.Series(word_list).value_counts().head(10)\n",
    "    word_freq_df = pd.DataFrame({'Word': word_freq.index, 'Frequency': word_freq.values})\n",
    "    print(f\"Word Frequencies for {supermarket.capitalize()}:\")\n",
    "    print(word_freq_df)\n",
    "\n",
    "for supermarket in selected_supermarkets:\n",
    "    generate_word_frequency_table(supermarket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d615b",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81456747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing of the models\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X = df['content']\n",
    "y = df['sentiment_category']\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_scores = cross_val_score(svm_classifier, X_tfidf, y, cv=5)  \n",
    "print(\"Support Vector Machine (SVM) Mean Accuracy:\", svm_scores.mean())\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "lr_scores = cross_val_score(logistic_regression, X_tfidf, y, cv=5) \n",
    "print(\"Logistic Regression Mean Accuracy:\", lr_scores.mean())\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "rf_scores = cross_val_score(random_forest, X_tfidf, y, cv=5) \n",
    "print(\"Random Forest Mean Accuracy:\", rf_scores.mean())\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "nb_scores = cross_val_score(naive_bayes, X_tfidf, y, cv=5) \n",
    "print(\"Naive Bayes Mean Accuracy:\", nb_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bde9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of the models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_true, y_pred, classifier_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "    plt.title(f'Confusion Matrix - {classifier_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "plot_confusion_matrix(y_test, y_pred_svm, 'SVM')\n",
    "plot_confusion_matrix(y_test, y_pred_lr, 'Logistic Regression')\n",
    "plot_confusion_matrix(y_test, y_pred_rf, 'Random Forest')\n",
    "plot_confusion_matrix(y_test, y_pred_nb, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df832c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing of hybrid model\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df['content']\n",
    "y = df['sentiment_category']\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train_labels, y_test_labels = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_model = SVC(kernel='linear', probability=True)\n",
    "svm_model.fit(X_train, y_train_labels)\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train_labels)\n",
    "\n",
    "svm_preds_test = svm_model.predict_proba(X_test) \n",
    "lr_preds_test = lr_model.predict_proba(X_test)\n",
    "\n",
    "X_test_combined = np.hstack((X_test.toarray(), svm_preds_test, lr_preds_test))\n",
    "\n",
    "hybrid_model = LogisticRegression()\n",
    "hybrid_model.fit(X_test_combined, y_test_labels)\n",
    "\n",
    "hybrid_accuracy = hybrid_model.score(X_test_combined, y_test_labels)\n",
    "print(\"Hybrid Model Accuracy:\", hybrid_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "hybrid_preds = hybrid_model.predict(X_test_combined)\n",
    "report = classification_report(y_test_labels, hybrid_preds)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onfusion matrix of the Hybrid model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "hybrid_preds = hybrid_model.predict(X_test_combined)\n",
    "\n",
    "cm = confusion_matrix(y_test_labels, hybrid_preds)\n",
    "# Plot confusion matrix for hybrid model\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=hybrid_model.classes_, yticklabels=hybrid_model.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix-Hybrid Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbad2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "hybrid_preds = hybrid_model.predict(X_test_combined)\n",
    "report = classification_report(y_test_labels, hybrid_preds)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
